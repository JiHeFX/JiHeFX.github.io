<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM应用（一）：使用Haytack搭建RAG | 及河の小屋</title><meta name="author" content="及河"><meta name="copyright" content="及河"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="记录RAG的学习过程">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM应用（一）：使用Haytack搭建RAG">
<meta property="og:url" content="https://jihefx.github.io/2025/03/04/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/index.html">
<meta property="og:site_name" content="及河の小屋">
<meta property="og:description" content="记录RAG的学习过程">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jihefx.github.io/album/picture/blackmyth/20240829164105_1.jpg">
<meta property="article:published_time" content="2025-03-04T07:51:13.000Z">
<meta property="article:modified_time" content="2025-07-08T07:06:17.538Z">
<meta property="article:author" content="及河">
<meta property="article:tag" content="RAG">
<meta property="article:tag" content="Haystack">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jihefx.github.io/album/picture/blackmyth/20240829164105_1.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM应用（一）：使用Haytack搭建RAG",
  "url": "https://jihefx.github.io/2025/03/04/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/",
  "image": "https://jihefx.github.io/album/picture/blackmyth/20240829164105_1.jpg",
  "datePublished": "2025-03-04T07:51:13.000Z",
  "dateModified": "2025-07-08T07:06:17.538Z",
  "author": [
    {
      "@type": "Person",
      "name": "及河",
      "url": "https://jihefx.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/Avatar_special_19.png"><link rel="canonical" href="https://jihefx.github.io/2025/03/04/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 8 || hour >= 22
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?f2944f39e681942bac974b69287698df";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"SFM6J5VR2H","apiKey":"28687f82ab4c42ae8f1068257df37c36","indexName":"hexo","hitsPerPage":6,"languages":{"input_placeholder":"请输入搜索内容......","hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: 及河","link":"链接: ","source":"来源: 及河の小屋","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM应用（一）：使用Haytack搭建RAG',
  isHighlightShrink: true,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="APlayer.min.css"><div id="aplayer"></div><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js" async></script><meta name="generator" content="Hexo 7.3.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="/css/loading-baar.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg" style="background-image: url(/album/picture/actwallpaper/138e84e7f0332cc685.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/Avatar_exusiai.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-file-alt"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-play-circle"></i><span> 分享</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-camera"></i><span> 电影</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> 书籍</span></a></li><li><a class="site-page child" href="/bangumis/"><i class="fa-fw fa-brands fa-youtube"></i><span> 番剧</span></a></li><li><a class="site-page child" href="/album/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open-text"></i><span> 留言</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/album/picture/blackmyth/20240829164105_1.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/Avatar_special_19.png" alt="Logo"><span class="site-name">及河の小屋</span></a><a class="nav-page-title" href="/"><span class="site-name">LLM应用（一）：使用Haytack搭建RAG</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-file-alt"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-play-circle"></i><span> 分享</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-camera"></i><span> 电影</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> 书籍</span></a></li><li><a class="site-page child" href="/bangumis/"><i class="fa-fw fa-brands fa-youtube"></i><span> 番剧</span></a></li><li><a class="site-page child" href="/album/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open-text"></i><span> 留言</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">LLM应用（一）：使用Haytack搭建RAG</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-04T07:51:13.000Z" title="发表于 2025-03-04 15:51:13">2025-03-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-08T07:06:17.538Z" title="更新于 2025-07-08 15:06:17">2025-07-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">5.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>24分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="twikoo_visitors"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:365,&quot;messagePrev&quot;:&quot;距离文章写完已有&quot;,&quot;messageNext&quot;:&quot;天，内容可能过期，请注意~&quot;,&quot;postUpdate&quot;:&quot;2025-07-08 15:06:17&quot;}" hidden></div><p>思考：是否需要将文档分为检索部分和知识部分？</p>
<p>Haystack：一个用于构建基于LLM的应用程序的框架，包括：</p>
<ul>
<li>Document：Haystack的核心数据类型</li>
<li>component：组件单元，用于实现一个功能，例如：清理文件、切分文档、生成嵌入、初步检索、重排序等等</li>
<li>pipeline：流水线，将组件串联起来，自动依次调用组件</li>
<li>document store：存储文档的向量数据库</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install haystack-ai </span><br></pre></td></tr></table></figure>
<h2 id="Document"><a href="#Document" class="headerlink" title="Document"></a>Document</h2><p>向量数据库的基本存储单元</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Document</span>(metaclass=_BackwardCompatible):</span><br><span class="line">	<span class="comment"># 文档的ID</span></span><br><span class="line">	<span class="built_in">id</span>: <span class="built_in">str</span> = field(default=<span class="string">&quot;&quot;</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 文档的内容</span></span><br><span class="line">	content: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="literal">None</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 检索和重排序时文档的分数</span></span><br><span class="line">	score: <span class="type">Optional</span>[<span class="built_in">float</span>] = field(default=<span class="literal">None</span>)</span><br><span class="line">	<span class="comment"># 文档的embedding，一个向量</span></span><br><span class="line">	embedding: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">float</span>]] = field(default=<span class="literal">None</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 文档的稀疏embedding，一个index-value的稀疏向量</span></span><br><span class="line">	sparse_embedding: <span class="type">Optional</span>[SparseEmbedding] = field(default=<span class="literal">None</span>) </span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 文档存在表格时，其pandas dataframe</span></span><br><span class="line">	dataframe: <span class="type">Optional</span>[DataFrame] = field(default=<span class="literal">None</span>) </span><br><span class="line">	<span class="comment"># 文档的二进制数据</span></span><br><span class="line">	blob: <span class="type">Optional</span>[ByteStream] = field(default=<span class="literal">None</span>)</span><br><span class="line">	<span class="comment"># 文档的元数据，例如文档名，文档作者，文档时间等</span></span><br><span class="line">	meta: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>] = field(default_factory=<span class="built_in">dict</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SparseEmbedding</span>:</span><br><span class="line">	indices: <span class="type">List</span>[<span class="built_in">int</span>]</span><br><span class="line">	values: <span class="type">List</span>[<span class="built_in">float</span>]</span><br></pre></td></tr></table></figure>

<h2 id="Component"><a href="#Component" class="headerlink" title="Component"></a>Component</h2><p>需要标明该组件单元的输入、输出和处理代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> haystack <span class="keyword">import</span> Document, component</span><br><span class="line"><span class="meta">@component</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DocumentCleaner</span>:</span><br><span class="line">        <span class="comment"># 输出在注解处标明</span></span><br><span class="line"><span class="meta">	@component.output_types(<span class="params">documents=<span class="type">List</span>[Document]</span>) </span></span><br><span class="line">        <span class="comment"># 输入在方法参数处标明</span></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self, documents: <span class="type">List</span>[Document]</span>): </span><br><span class="line">		<span class="comment"># 处理代码在run()方法里</span></span><br><span class="line">		cleaned_docs = clean(documents) <span class="comment"># 清理文档内容...</span></span><br><span class="line">                <span class="comment"># 返回值必须是一个字典，key是和输出注解同名</span></span><br><span class="line">                <span class="comment"># 的字符串，value是清理好的文档列表</span></span><br><span class="line">		<span class="keyword">return</span> &#123;<span class="string">&quot;documents&quot;</span>: cleaned_docs&#125; </span><br></pre></td></tr></table></figure>

<h2 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h2><ul>
<li>将定义好的组件单元(component)添加到流水线中</li>
<li>将组件单元连接起来，一般情况下都是有向无环图</li>
<li>组件单元的输入即可以来自内部的前一个组件的输出，也可以来自外部传入的数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Pipeline</span>:</span><br><span class="line">	<span class="comment"># 启动流水线</span></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">run</span>(<span class="params"></span></span><br><span class="line"><span class="params">		self, </span></span><br><span class="line"><span class="params">        <span class="comment"># 外部传入的数据，通过字典指定要给哪个组件传入哪个输入</span></span></span><br><span class="line"><span class="params">		data: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>], </span></span><br><span class="line"><span class="params">        <span class="comment"># 流水线的最终输出，通过字典说明哪个组件的哪个输出</span></span></span><br><span class="line"><span class="params">	</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]: </span><br><span class="line">	  </span><br><span class="line">	<span class="comment"># 将组件instance加入流水线，并命名为name</span></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">add_component</span>(<span class="params">self, name: <span class="built_in">str</span>, instance: Component</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 将两个组件的输入和输出连接起来</span></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">connect</span>(<span class="params">self, sender: <span class="built_in">str</span>, receiver: <span class="built_in">str</span></span>)</span><br></pre></td></tr></table></figure>

<h2 id="Converter"><a href="#Converter" class="headerlink" title="Converter"></a>Converter</h2><p>haystack有一些可以将其他格式的文件转换成Document的组件，比如文本文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> haystack.components.converters <span class="keyword">import</span> TextFileToDocument</span><br><span class="line"></span><br><span class="line">converter = TextFileToDocument()</span><br><span class="line">docs = converter.run(sources=[<span class="string">&quot;./files/hello_world.txt&quot;</span>])[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;id: <span class="subst">&#123;docs[<span class="number">0</span>].<span class="built_in">id</span>&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;content: <span class="subst">&#123;docs[<span class="number">0</span>].content&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;docs[<span class="number">0</span>].score&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;embedding: <span class="subst">&#123;docs[<span class="number">0</span>].embedding&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;sparse_embedding: <span class="subst">&#123;docs[<span class="number">0</span>].sparse_embedding&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;meta: <span class="subst">&#123;docs[<span class="number">0</span>].meta&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>TextFileToDocument：一个component组件，输入是知识库文件路径列表list[Path]，输出是一个文档列表list[Document]</li>
<li>输出结果：（没有embedding处理流程和检索流程，所以没有embedding和score）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">id: 85a2d0b785a183d80b84c23f2325d6e0dcda73da7304f0dc217ac55f2eb1a0b8</span><br><span class="line">content: Hello World!</span><br><span class="line">score: None</span><br><span class="line">embedding: None</span><br><span class="line">sparse_embedding: None</span><br><span class="line">meta: &#123;&#x27;file_path&#x27;: &#x27;hello_world.txt&#x27;&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Spliter"><a href="#Spliter" class="headerlink" title="Spliter"></a>Spliter</h2><p>文档切分是RAG的第一个核心点，目前主流有两种方式：直接切分和语法切分</p>
<p>在介绍具体切分方法之前，需要回答：什么样的文档块是好的？</p>
<ul>
<li><p>文档块长度需要适中，这个长度不好拿捏</p>
<ul>
<li>长文档块的缺点：</li>
</ul>
<ol>
<li>输入上下文增大，降低回答质量</li>
<li>信息量过多，检索准确度降低</li>
<li>信息量过多，正确的参考信息被太多无关信息淹没，大模型找不到对应的内容(影响attention计算的精确度)</li>
</ol>
<ul>
<li>短文档块的缺点：</li>
</ul>
<ol>
<li>信息量过少，大模型找不到参考信息</li>
<li>文档数量提升，降低检索速度</li>
<li>更多的语义碎片，丢失语义连贯性和长文本中的实体依赖关系，俗称“说话说一半”</li>
</ol>
<ul>
<li>文档块的内容要全面：但往往全面的文档块会很长，所以更重要的是如何在保证文档块长度适中的情况下，把“说话说一半”提升到“说话说四分之三”*</li>
<li>文档块的长度要平均：尽量保证所有文档块的长度都差不多长。因为在计算相似度分数时，嵌入模型会更倾向于给短文档块打更高的分数</li>
</ul>
</li>
</ul>
<h3 id="DocumentSplitter"><a href="#DocumentSplitter" class="headerlink" title="DocumentSplitter"></a>DocumentSplitter</h3><ul>
<li>split_by：常用的基本单位有page、passage、sentence、line、word，这里我们以词(word)为基本单位进行切分。哪个基本单位好呢？<ul>
<li>word看起来很好，因为它可以保证所有的文档块都一样长，足够平均；但在头尾处会出现严重的不连贯现象</li>
<li>page和passage则是的文档块长度分布不均，以及超长文档块的出现</li>
<li>所以一般而言sentence或line是个不错的选择</li>
</ul>
</li>
<li>split_length：切分的基本长度</li>
<li>split_overlap：为了减少“说话说一半”的情况出现，让文档块之间相互重叠。假如2 3是连贯内容，重叠就可以使得它们连起来；不重叠则会被切断<br><img src="/pic/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/overlap.png" alt="图片错误"></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">numbers = <span class="string">&quot;0 1 2 3 4 5 6 7 8 9&quot;</span></span><br><span class="line">document = Document(content=numbers)</span><br><span class="line">splitter = DocumentSplitter(split_by=<span class="string">&quot;word&quot;</span>, split_length=<span class="number">3</span>, split_overlap=<span class="number">1</span>)</span><br><span class="line">docs = splitter.run(documents=[document])[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;document: <span class="subst">&#123;document.content&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> index,doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(docs):</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&quot;document_<span class="subst">&#123;index&#125;</span>: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">document: 0 1 2 3 4 5 6 7 8 9</span><br><span class="line">document_0: 0 1 2 </span><br><span class="line">document_1: 2 3 4 </span><br><span class="line">document_2: 4 5 6 </span><br><span class="line">document_3: 6 7 8 </span><br><span class="line">document_4: 8 9</span><br></pre></td></tr></table></figure>
<h3 id="NLTKDocumentSplitter"><a href="#NLTKDocumentSplitter" class="headerlink" title="NLTKDocumentSplitter"></a>NLTKDocumentSplitter</h3><p>奇怪的输入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> haystack.components.preprocessors <span class="keyword">import</span> NLTKDocumentSplitter, DocumentSplitter</span><br><span class="line"><span class="keyword">from</span> haystack <span class="keyword">import</span> Document</span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;&quot;&quot;The dog was called Wellington. It belonged to Mrs. Shears who was our friend. </span></span><br><span class="line"><span class="string">She lived on the opposite side of the road, two houses to the left.&quot;&quot;&quot;</span></span><br><span class="line">document = Document(content=text)</span><br></pre></td></tr></table></figure>
<p>简单以句子为单位切分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">simple_splitter = DocumentSplitter(split_by=<span class="string">&quot;sentence&quot;</span>, split_length=<span class="number">1</span>, split_overlap=<span class="number">0</span>)</span><br><span class="line">simple_docs = simple_splitter.run(documents=[document])[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nsimple:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> index, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(simple_docs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;document_<span class="subst">&#123;index&#125;</span>: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">simple:</span><br><span class="line">document_0: The dog was called Wellington.</span><br><span class="line">document_1:  It belonged to Mrs.</span><br><span class="line">document_2:  Shears who was our friend.</span><br><span class="line">document_3:  She lived on the opposite side of the road, two houses to the left.</span><br></pre></td></tr></table></figure>
<p>无法区分 Mrs. Shears的点号和句号，所以我们需要nltk来对单词和符号进行tag标注</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nltk_splitter = NLTKDocumentSplitter(split_by=<span class="string">&quot;sentence&quot;</span>, split_length=<span class="number">1</span>, split_overlap=<span class="number">0</span>)</span><br><span class="line">nltk_docs = nltk_splitter.run(documents=[document])[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nnltk:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> index, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(nltk_docs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;document_<span class="subst">&#123;index&#125;</span>: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nltk:</span><br><span class="line">document_0: The dog was called Wellington. </span><br><span class="line">document_1: It belonged to Mrs. Shears who was our friend. </span><br><span class="line">document_2: She lived on the opposite side of the road, two houses to the left.</span><br></pre></td></tr></table></figure>
<h2 id="Retreiver"><a href="#Retreiver" class="headerlink" title="Retreiver"></a>Retreiver</h2><h3 id="BM25Retriever原理"><a href="#BM25Retriever原理" class="headerlink" title="BM25Retriever原理"></a>BM25Retriever原理</h3><p>BM25是搜索引擎领域计算查询与文档相关性的排名函数，它是一种基于词袋的检索函数：通过统计查询和文档的单词匹配数量来计算二者相似度分数<br><img src="/pic/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/bm25.png" alt="图片错误"><br>其中：</p>
<ul>
<li>查询$Q$包含关键字$q_1,…,q_n$</li>
<li>$f(q_i,D)$是$q_i$在文档$D$中的词频</li>
<li>$|D|$是文档长度</li>
<li>$avgdl$是平均文档长度;$IDF(q_i)$是$q_i$的逆向文档频率权重;$k_1$和$b$是超参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">document_store = InMemoryDocumentStore()</span><br><span class="line">documents = [</span><br><span class="line">	Document(content=<span class="string">&quot;There are over 7,000 languages spoken around the world today.&quot;</span>),</span><br><span class="line">	Document(content=<span class="string">&quot;Elephants have been observed to behave in a way that indicates a high level of self-awareness, such as recognizing themselves in mirrors.&quot;</span>),</span><br><span class="line">	Document(content=<span class="string">&quot;In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the phenomenon of bioluminescent waves.&quot;</span>)</span><br><span class="line">]</span><br><span class="line">document_store.write_documents(documents=documents)</span><br><span class="line">retriever = InMemoryBM25Retriever(document_store=document_store)</span><br><span class="line">docs = retriever.run(query=<span class="string">&quot;How many languages are spoken around the world today?&quot;</span>)[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> docs:</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&quot;content: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;doc.score&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">content: There are over 7,000 languages spoken around the world today.</span><br><span class="line">score: 7.815769833242408</span><br><span class="line">content: In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the phenomenon of bioluminescent waves.</span><br><span class="line">score: 4.314753296196667</span><br><span class="line">content: Elephants have been observed to behave in a way that indicates a high level of self-awareness, such as recognizing themselves in mirrors.</span><br><span class="line">score: 3.652595952218814</span><br></pre></td></tr></table></figure>
<ul>
<li>优缺点<ul>
<li>速度快：基于统计的分数计算公式很简单，可以快速处理大规模文本数据</li>
<li>存储开销小：除文本外无需存储额外数据。如果下游大模型通过API调用，rag不需要显卡也能跑起来，而且很快</li>
<li>太依赖关键字：query质量不高就搜不到，无法捕获文本的上下文语义信息。比如，在搜索引擎中，如果不输入关键字那必然搜不到我们想要的内容</li>
</ul>
</li>
</ul>
<h3 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h3><p>最近几年，一种基于BERT架构衍生出来的多种语义检索技术被更多地用到了RAG中，他是一种encoder-only的transformer架构：</p>
<ul>
<li>Tokenizer：words -&gt; tokens</li>
<li>Embedding：tokens -&gt; vectors</li>
<li>Encoder Stack：vectors -&gt; vectors</li>
</ul>
<p>简言之，它可以将文本转换成若干token vector<br><img src="/pic/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/BERT.png" alt="图片错误"></p>
<h3 id="DenseEmbeddingRetriever-文本嵌入模型"><a href="#DenseEmbeddingRetriever-文本嵌入模型" class="headerlink" title="DenseEmbeddingRetriever: 文本嵌入模型"></a>DenseEmbeddingRetriever: 文本嵌入模型</h3><p>密集嵌入检索器基于双编码器(Bi-Encoder)架构，在BERT上面外加一层池化层(Pooling)，得到单一的句向量，存储到document.embedding中。</p>
<ul>
<li>sentence -&gt;BERT-Encoder -&gt; token vectors</li>
<li>token vectors -&gt; Pooling Layer -&gt; sentence vector</li>
<li>score(SentenceA, SentenceB) &#x3D; cosine_similarity(vectorA,vectorB)<br><img src="/pic/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/bi-encoder.png" alt="图片错误"></li>
</ul>
<h3 id="DenseEmbeddingRetriever-相似度计算"><a href="#DenseEmbeddingRetriever-相似度计算" class="headerlink" title="DenseEmbeddingRetriever: 相似度计算"></a>DenseEmbeddingRetriever: 相似度计算</h3><p>密集向量会交给一个经过训练的嵌入模型生成，它可以将相似的句子映射到高维空间中距离相近、方向相似的向量，常用的相似度分数计算公式有两种：</p>
<ul>
<li>余弦相似度：常用的相似度计算公式，计算两个向量之间的夹角的余弦值。两个向量的方向越一致相似度越高<br>$$<br>CosineSimilarity&#x3D;\frac{A·B}{||A||\space||B||}&#x3D;\frac{\sum_{i&#x3D;1}^nA_iB_i}{\sqrt{\sum_{i&#x3D;1}^nA_iA_i}·\sqrt{\sum_{i&#x3D;1}^nB_iB_i}}<br>$$</li>
<li>欧式似度：直接计算两个向量之间的欧几里得距离，然后取个倒数得到相似度分数。也可以用其他距离：曼哈顿距离、汉明距离等<br>$$<br>EuclideanSimilarity&#x3D;\frac{1}{1+\sqrt{\sum_{i&#x3D;1}^n(A_i-B_i)^2}}<br>$$</li>
</ul>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><ul>
<li>模型: sentence-transformers&#x2F;all-MiniLM-L6-v2, 22.7M params</li>
<li>相似度分数：余弦相似度</li>
</ul>
<h5 id="处理查询"><a href="#处理查询" class="headerlink" title="处理查询"></a>处理查询</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">document_store = InMemoryDocumentStore(embedding_similarity_function=<span class="string">&quot;cosine&quot;</span>)</span><br><span class="line">document_embedder = SentenceTransformersDocumentEmbedder(</span><br><span class="line">	model=<span class="string">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span></span><br><span class="line">)</span><br><span class="line">document_embedder.warm_up()</span><br><span class="line">documents_with_embeddings = document_embedder.run(documents)[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line">document_store.write_documents(documents_with_embeddings)</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> documents_with_embeddings:</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&quot;content: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;doc.score&#125;</span>&quot;</span>)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&quot;embedding: <span class="subst">&#123;doc.embedding&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">query_pipeline = Pipeline()</span><br><span class="line">query_pipeline.add_component(</span><br><span class="line">	<span class="string">&quot;text_embedder&quot;</span>,</span><br><span class="line">	SentenceTransformersTextEmbedder(model=<span class="string">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span>),</span><br><span class="line">)</span><br><span class="line">query_pipeline.add_component(</span><br><span class="line">	<span class="string">&quot;retriever&quot;</span>, InMemoryEmbeddingRetriever(document_store=document_store)</span><br><span class="line">)</span><br><span class="line">query_pipeline.connect(<span class="string">&quot;text_embedder.embedding&quot;</span>, <span class="string">&quot;retriever.query_embedding&quot;</span>)</span><br><span class="line"></span><br><span class="line">query = <span class="string">&quot;How many languages are there?&quot;</span></span><br><span class="line">result = query_pipeline.run(&#123;<span class="string">&quot;text_embedder&quot;</span>: &#123;<span class="string">&quot;text&quot;</span>: query&#125;&#125;)</span><br><span class="line">result_documents = result[<span class="string">&quot;retriever&quot;</span>][<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> result_documents:</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&quot;content: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;doc.score&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">content: There are over 7,000 languages spoken around the world today.</span><br><span class="line">score: 0.7557791972968138</span><br><span class="line"></span><br><span class="line">content: Elephants have been observed to behave in a way that indicates a high level of self-awareness, such as recognizing themselves in mirrors.</span><br><span class="line">score: 0.04221229186262071</span><br><span class="line"></span><br><span class="line">content: In certain parts of the world, like the Maldives, Puerto Rico, and San Diego, you can witness the phenomenon of bioluminescent waves.</span><br><span class="line">score: -0.001667825878752048</span><br></pre></td></tr></table></figure>
<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><ul>
<li>速度快：可以提前在GPU上计算并存储文档块的dense embedding，计算相似度就会很快</li>
<li>存储开销小：每个文档块只需要额外存储一个高维向量(通常768或1024维)</li>
<li>捕获句子的语义信息：只要是相似的句子，关键字不匹配也可以检索到</li>
<li>丢失词元信息：BERT产生的众多词元向量全部被映射到单一句向量，丢失了很多文本中的细节。快速地粗读文本，速度虽快但忽略了细节，只了解了个大概</li>
</ul>
<h3 id="SimilarityReranker-相似度计算模型"><a href="#SimilarityReranker-相似度计算模型" class="headerlink" title="SimilarityReranker: 相似度计算模型"></a>SimilarityReranker: 相似度计算模型</h3><ul>
<li><p>similarity reranker基于交叉编码器(cross-encoder)架构</p>
</li>
<li><p>直接将两个句子串联起来，交给BERT，使得两个句子的词元向量可以在BERT内部相互交叉(cross)地进行交互，最终经过softmax得到一个相似度分数<br><img src="/pic/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/reranker.png" alt="图片错误"></p>
</li>
<li><p>cross vs. colbert: 词元向量的交互从相似度计算阶段(colbert)，提前到BERT模型内部(cross)<br><img src="/pic/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/cross-vs-colbert.png" alt="图片错误"></p>
</li>
</ul>
<h4 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> haystack <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">from</span> haystack.components.rankers <span class="keyword">import</span> TransformersSimilarityRanker</span><br><span class="line"></span><br><span class="line">documents = [</span><br><span class="line">    Document(content=<span class="string">&quot;There are over 7,000 languages spoken around the world today.&quot;</span>),</span><br><span class="line">    Document(content=<span class="string">&quot;Elephants have been observed to behave in a way that indicates </span></span><br><span class="line"><span class="string">    a high level of self-awareness, such as recognizing themselves in mirrors.&quot;</span>),</span><br><span class="line">    Document(content=<span class="string">&quot;In certain parts of the world, like the Maldives, Puerto Rico, </span></span><br><span class="line"><span class="string">    and San Diego, you can witness the phenomenon of bioluminescent waves.&quot;</span>),</span><br><span class="line">]</span><br><span class="line">ranker = TransformersSimilarityRanker(model=<span class="string">&quot;cross-encoder/ms-marco-MiniLM-L-6-v2&quot;</span>)</span><br><span class="line">ranker.warm_up()</span><br><span class="line">query = <span class="string">&quot;How many languages are there?&quot;</span></span><br><span class="line">ranked_documents = ranker.run(query=query, documents=documents)[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> ranked_documents:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;content: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;doc.score&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="输出-1"><a href="#输出-1" class="headerlink" title="输出"></a>输出</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">content: There are over 7,000 languages spoken around the world today.</span><br><span class="line">score: 0.9998884201049805</span><br><span class="line"></span><br><span class="line">content: Elephants have been observed to behave in a way that indicates </span><br><span class="line">a high level of self-awareness, such as recognizing themselves in mirrors.</span><br><span class="line">score: 1.4616251974075567e-05</span><br><span class="line"></span><br><span class="line">content: In certain parts of the world, like the Maldives, Puerto Rico, </span><br><span class="line">and San Diego, you can witness the phenomenon of bioluminescent waves.</span><br><span class="line">score: 1.4220857337932102e-05</span><br></pre></td></tr></table></figure>
<h3 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h3><ul>
<li>充分利用词元信息：相似度直接在模型内部完成计算。同时看两个文本，交叉理解两个文本的单词的含义，训练好的模型可以得到很好的相似度计算结果</li>
<li>在线计算：所有的计算都要在GPU上在线完成，无法提前存储一些信息，实现之前的离线计算，因此会很慢</li>
</ul>
<h2 id="Simple-RAG"><a href="#Simple-RAG" class="headerlink" title="Simple RAG"></a>Simple RAG</h2><p>挑一种文档划分方法，再挑一个检索器，一个简单的RAG就可以完成了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> prompt_toolkit <span class="keyword">import</span> prompt</span><br><span class="line"><span class="keyword">from</span> haystack <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> haystack.utils <span class="keyword">import</span> Secret</span><br><span class="line"><span class="keyword">from</span> haystack.document_stores.in_memory <span class="keyword">import</span> InMemoryDocumentStore</span><br><span class="line"><span class="keyword">from</span> haystack.components.fetchers <span class="keyword">import</span> LinkContentFetcher</span><br><span class="line"><span class="keyword">from</span> haystack.components.converters <span class="keyword">import</span> HTMLToDocument</span><br><span class="line"><span class="keyword">from</span> haystack.components.preprocessors <span class="keyword">import</span> DocumentSplitter</span><br><span class="line"><span class="keyword">from</span> haystack.components.writers <span class="keyword">import</span> DocumentWriter</span><br><span class="line"><span class="keyword">from</span> haystack.components.retrievers.in_memory <span class="keyword">import</span> InMemoryEmbeddingRetriever</span><br><span class="line"><span class="keyword">from</span> haystack.components.generators <span class="keyword">import</span> OpenAIGenerator</span><br><span class="line"><span class="keyword">from</span> haystack.components.builders.prompt_builder <span class="keyword">import</span> PromptBuilder</span><br><span class="line"><span class="keyword">from</span> haystack.components.embedders <span class="keyword">import</span> (</span><br><span class="line">    SentenceTransformersTextEmbedder, SentenceTransformersDocumentEmbedder,)</span><br></pre></td></tr></table></figure>
<h3 id="处理文档"><a href="#处理文档" class="headerlink" title="处理文档"></a>处理文档</h3><ul>
<li>使用sentence-transformers&#x2F;all-MiniLM-L6-v2嵌入模型进行检索</li>
<li>以3行为单位进行切分，并且有1行的overlap</li>
<li>将南京大学的wiki网页作为知识库：<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Nanjing_University">https://en.wikipedia.org/wiki/Nanjing_University</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">document_store = InMemoryDocumentStore()</span><br><span class="line">fetcher = LinkContentFetcher()</span><br><span class="line">converter = HTMLToDocument()</span><br><span class="line">splitter = DocumentSplitter(split_by=<span class="string">&quot;sentence&quot;</span>, split_length=<span class="number">3</span>, split_overlap=<span class="number">1</span>)</span><br><span class="line">document_embedder = SentenceTransformersDocumentEmbedder(</span><br><span class="line">    model=<span class="string">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span></span><br><span class="line">)</span><br><span class="line">writer = DocumentWriter(document_store = document_store)</span><br><span class="line"></span><br><span class="line">indexing_pipeline = Pipeline()</span><br><span class="line">indexing_pipeline.add_component(<span class="string">&quot;fetcher&quot;</span>, fetcher)</span><br><span class="line">indexing_pipeline.add_component(<span class="string">&quot;converter&quot;</span>, converter)</span><br><span class="line">indexing_pipeline.add_component(<span class="string">&quot;splitter&quot;</span>, splitter)</span><br><span class="line">indexing_pipeline.add_component(<span class="string">&quot;document_embedder&quot;</span>, document_embedder)</span><br><span class="line">indexing_pipeline.add_component(<span class="string">&quot;writer&quot;</span>, writer)</span><br><span class="line"></span><br><span class="line">indexing_pipeline.connect(<span class="string">&quot;fetcher.streams&quot;</span>, <span class="string">&quot;converter.sources&quot;</span>)</span><br><span class="line">indexing_pipeline.connect(<span class="string">&quot;converter.documents&quot;</span>, <span class="string">&quot;splitter.documents&quot;</span>)</span><br><span class="line">indexing_pipeline.connect(<span class="string">&quot;splitter.documents&quot;</span>, <span class="string">&quot;document_embedder.documents&quot;</span>)</span><br><span class="line">indexing_pipeline.connect(<span class="string">&quot;document_embedder.documents&quot;</span>, <span class="string">&quot;writer.documents&quot;</span>)</span><br><span class="line"></span><br><span class="line">indexing_pipeline.run(data=&#123;<span class="string">&quot;fetcher&quot;</span>: &#123;<span class="string">&quot;urls&quot;</span>: [<span class="string">&quot;https://en.wikipedia.org/wiki/Nanjing_University&quot;</span>]&#125;&#125;)</span><br></pre></td></tr></table></figure>

<h3 id="处理查询-1"><a href="#处理查询-1" class="headerlink" title="处理查询"></a>处理查询</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">prompt_template = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Given these documents, answer the question.</span></span><br><span class="line"><span class="string">Documents:</span></span><br><span class="line"><span class="string">&#123;% for doc in documents %&#125;</span></span><br><span class="line"><span class="string">    &#123;&#123; doc.content &#125;&#125;</span></span><br><span class="line"><span class="string">&#123;% endfor %&#125;</span></span><br><span class="line"><span class="string">Question: &#123;&#123;question&#125;&#125;</span></span><br><span class="line"><span class="string">Answer:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">api_key = <span class="string">&quot;xxx&quot;</span></span><br><span class="line">model = <span class="string">&quot;gpt-4o-mini&quot;</span></span><br><span class="line">api_base_url = <span class="literal">None</span></span><br><span class="line">query_embedder = SentenceTransformersTextEmbedder(model=<span class="string">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span>)</span><br><span class="line">retriever = InMemoryEmbeddingRetriever(document_store=document_store)</span><br><span class="line">prompt_builder = PromptBuilder(template=prompt_template)</span><br><span class="line">llm = OpenAIGenerator(</span><br><span class="line">    api_key=Secret.from_token(api_key),</span><br><span class="line">    model=model,</span><br><span class="line">    api_base_url=api_base_url</span><br><span class="line">)</span><br><span class="line">rag_pipeline = Pipeline()</span><br><span class="line">rag_pipeline.add_component(<span class="string">&quot;query_embedder&quot;</span>, query_embedder)</span><br><span class="line">rag_pipeline.add_component(<span class="string">&quot;retriever&quot;</span>, retriever)</span><br><span class="line">rag_pipeline.add_component(<span class="string">&quot;prompt_builder&quot;</span>, prompt_builder)</span><br><span class="line">rag_pipeline.add_component(<span class="string">&quot;llm&quot;</span>, llm)</span><br><span class="line">rag_pipeline.connect(<span class="string">&quot;query_embedder.embedding&quot;</span>, <span class="string">&quot;retriever.query_embedding&quot;</span>)</span><br><span class="line">rag_pipeline.connect(<span class="string">&quot;retriever.documents&quot;</span>, <span class="string">&quot;prompt_builder.documents&quot;</span>)</span><br><span class="line">rag_pipeline.connect(<span class="string">&quot;prompt_builder.prompt&quot;</span>, <span class="string">&quot;llm.prompt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">True</span>):</span><br><span class="line">    question = prompt(<span class="string">&quot;&gt; &quot;</span>)</span><br><span class="line">    results = rag_pipeline.run(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;query_embedder&quot;</span>: &#123;<span class="string">&quot;text&quot;</span>: question&#125;,</span><br><span class="line">            <span class="string">&quot;prompt_builder&quot;</span>: &#123;<span class="string">&quot;question&quot;</span>: question&#125;,</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    reply = results[<span class="string">&quot;llm&quot;</span>][<span class="string">&quot;replies&quot;</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(reply)</span><br></pre></td></tr></table></figure>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><h4 id="What-is-the-motto-of-Nanjing-University？"><a href="#What-is-the-motto-of-Nanjing-University？" class="headerlink" title="What is the motto of Nanjing University？"></a>What is the motto of Nanjing University？</h4><p>The motto of Nanjing University is “诚朴雄伟励学敦行,” which translates to “Sincerity with Aspiration, Perseverance and Integrity” in English. The first half of this motto was the motto during the National Central University time, and the last half was quoted from the classic literature work Book of Rites.<br><img src="/pic/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/motto.png" alt="图片错误"></p>
<h4 id="What-is-the-song-of-Nanjing-University？"><a href="#What-is-the-song-of-Nanjing-University？" class="headerlink" title="What is the song of Nanjing University？"></a>What is the song of Nanjing University？</h4><p>The song of Nanjing University is the university song, which was created in 1916. It is the first school song in the modern history of Nanjing University. The lyrics were written by Jiang Qian, and the melody was composed by Li Shutong. The song was recovered in 2002.<br><img src="/pic/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/song.png" alt="图片错误"></p>
<h4 id="问一些大模型不知道的问题"><a href="#问一些大模型不知道的问题" class="headerlink" title="问一些大模型不知道的问题"></a>问一些大模型不知道的问题</h4><p>question: Who is the modern China’s first PhD in Chinese Language and Literature?</p>
<h5 id="Chatgpt-answer"><a href="#Chatgpt-answer" class="headerlink" title="Chatgpt answer"></a>Chatgpt answer</h5><p>一会说1986年的郭齐勇，一会说1983年的陈平原<br><img src="/pic/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/gpt.png" alt="图片错误"></p>
<h5 id="RAG-answer"><a href="#RAG-answer" class="headerlink" title="RAG answer"></a>RAG answer</h5><p>The modern China’s first PhD in Chinese Language and Literature is Mo Lifeng (莫砺锋), as mentioned in the documents.<br><img src="/pic/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/phd.png" alt="图片错误"></p>
<h2 id="Advanced-RAG-检索结果合并"><a href="#Advanced-RAG-检索结果合并" class="headerlink" title="Advanced RAG: 检索结果合并"></a>Advanced RAG: 检索结果合并</h2><p>不同的检索器有不同的侧重点，会得到不同的相似度分数分布，如何综合考虑？例如一本书我既想略读整体(dense embedding)，也想跳着读重点部分(sparse embedding)</p>
<h3 id="权重合并-Weight-Merge"><a href="#权重合并-Weight-Merge" class="headerlink" title="权重合并(Weight Merge)"></a>权重合并(Weight Merge)</h3><p>$$\alpha·scale(s_1)+(1-\alpha)·scale(s_2)$$</p>
<ul>
<li>两种检索机制的分数的值域、分布不一致，通过放缩补偿</li>
<li>通过加权和计算综合分数</li>
</ul>
<h3 id="RRF-倒排融合"><a href="#RRF-倒排融合" class="headerlink" title="RRF(倒排融合)"></a>RRF(倒排融合)</h3><p>$$RRFscore(d\in D)&#x3D;\sum_{r\in R}\frac{1}{k+r(d)}$$</p>
<ul>
<li>只考虑文档在排序中的位置，忽略分数分布</li>
<li>$r(d)$是文档d在一种检索机制下的排序</li>
<li>k是超参</li>
</ul>
<h4 id="例子-2"><a href="#例子-2" class="headerlink" title="例子"></a>例子</h4><h5 id="import"><a href="#import" class="headerlink" title="import"></a>import</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> haystack <span class="keyword">import</span> Document, Pipeline</span><br><span class="line"><span class="keyword">from</span> haystack.document_stores.in_memory <span class="keyword">import</span> InMemoryDocumentStore</span><br><span class="line"><span class="keyword">from</span> haystack.components.embedders <span class="keyword">import</span> (</span><br><span class="line">    SentenceTransformersTextEmbedder,</span><br><span class="line">    SentenceTransformersDocumentEmbedder,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> haystack.components.retrievers.in_memory <span class="keyword">import</span> InMemoryBM25Retriever</span><br><span class="line"><span class="keyword">from</span> haystack.components.retrievers <span class="keyword">import</span> InMemoryEmbeddingRetriever</span><br><span class="line"><span class="keyword">from</span> haystack.components.joiners.document_joiner <span class="keyword">import</span> DocumentJoiner</span><br></pre></td></tr></table></figure>
<h5 id="文档处理"><a href="#文档处理" class="headerlink" title="文档处理"></a>文档处理</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">document_store = InMemoryDocumentStore(embedding_similarity_function=<span class="string">&quot;cosine&quot;</span>)</span><br><span class="line"></span><br><span class="line">query = <span class="string">&quot;What are effective strategies to improve English speaking skills?&quot;</span></span><br><span class="line">documents = [</span><br><span class="line">    Document(content=<span class="string">&quot;Practicing with native speakers enhances English </span></span><br><span class="line"><span class="string">                      speaking proficiency.&quot;</span>),</span><br><span class="line">    Document(content=<span class="string">&quot;Regular participation in debates and discussions </span></span><br><span class="line"><span class="string">                      refine public speaking skills in English.&quot;</span>),</span><br><span class="line">    Document(content=<span class="string">&quot;Studying the history of the English language does</span></span><br><span class="line"><span class="string">                      not directly improve speaking skills.&quot;</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">document_embedder = SentenceTransformersDocumentEmbedder(</span><br><span class="line">    model=<span class="string">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span></span><br><span class="line">)</span><br><span class="line">document_embedder.warm_up()</span><br><span class="line">documents_with_embeddings = document_embedder.run(documents)[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line">document_store.write_documents(documents_with_embeddings)</span><br></pre></td></tr></table></figure>
<h5 id="bm25检索"><a href="#bm25检索" class="headerlink" title="bm25检索"></a>bm25检索</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bm25_retriever = InMemoryBM25Retriever(document_store=document_store，scale_score=<span class="literal">True</span>)</span><br><span class="line">bm25_docs = bm25_retriever.run(query=query)[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;bm25:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> bm25_docs:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;content: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;doc.score&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="输出-2"><a href="#输出-2" class="headerlink" title="输出"></a>输出</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">content: Studying the history of the English language does not directly improve </span><br><span class="line">speaking skills.</span><br><span class="line">score: 0.5593245377361279</span><br><span class="line">content: Regular participation in debates and discussions refine public speaking </span><br><span class="line">skills in English.</span><br><span class="line">score: 0.545159185512614</span><br><span class="line">content: Practicing with native speakers enhances English speaking proficiency.</span><br><span class="line">score: 0.5387709786621966</span><br></pre></td></tr></table></figure>
<h5 id="dense-embedding检索"><a href="#dense-embedding检索" class="headerlink" title="dense embedding检索"></a>dense embedding检索</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">query_pipeline = Pipeline()</span><br><span class="line">query_pipeline.add_component(</span><br><span class="line">    <span class="string">&quot;text_embedder&quot;</span>,</span><br><span class="line">    SentenceTransformersTextEmbedder(model=<span class="string">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span>),</span><br><span class="line">)</span><br><span class="line">query_pipeline.add_component(</span><br><span class="line">    <span class="string">&quot;dense_retriever&quot;</span>, InMemoryEmbeddingRetriever(document_store=document_store，scale_score=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line">query_pipeline.connect(<span class="string">&quot;text_embedder.embedding&quot;</span>, <span class="string">&quot;dense_retriever.query_embedding&quot;</span>)</span><br><span class="line">dense_docs = query_pipeline.run(&#123;<span class="string">&quot;text_embedder&quot;</span>: &#123;<span class="string">&quot;text&quot;</span>: query&#125;&#125;)[<span class="string">&quot;dense_retriever&quot;</span>][<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dense:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> dense_docs:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;content: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;doc.score&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="输出-3"><a href="#输出-3" class="headerlink" title="输出"></a>输出</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">content: Practicing with native speakers enhances English speaking proficiency.</span><br><span class="line">score: 0.8296398226909952</span><br><span class="line"></span><br><span class="line">content: Regular participation in debates and discussions refine public speaking </span><br><span class="line">skills in English.</span><br><span class="line">score: 0.8017774366152697</span><br><span class="line"></span><br><span class="line">content: Studying the history of the English language does not directly improve </span><br><span class="line">speaking skills.</span><br><span class="line">score: 0.7334273104138469</span><br></pre></td></tr></table></figure>
<h5 id="权重合并"><a href="#权重合并" class="headerlink" title="权重合并"></a>权重合并</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">joiner = DocumentJoiner(join_mode=<span class="string">&quot;merge&quot;</span>, weights=[<span class="number">0.3</span>, <span class="number">0.7</span>])</span><br><span class="line">merge_docs = joiner.run(documents=[bm25_docs, dense_docs])[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> merge_docs:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;content: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;doc.score&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="输出-4"><a href="#输出-4" class="headerlink" title="输出"></a>输出</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">content: Practicing with native speakers enhances English speaking proficiency.</span><br><span class="line">score: 0.7423791694823556</span><br><span class="line">content: Regular participation in debates and discussions refine public speaking </span><br><span class="line">skills in English.</span><br><span class="line">score: 0.724791961284473</span><br><span class="line">content: Studying the history of the English language does not directly improve </span><br><span class="line">speaking skills.</span><br><span class="line">score: 0.6811964786105311</span><br></pre></td></tr></table></figure>
<h5 id="RRF合并"><a href="#RRF合并" class="headerlink" title="RRF合并"></a>RRF合并</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">joiner = DocumentJoiner(join_mode=<span class="string">&quot;reciprocal_rank_fusion&quot;</span>)</span><br><span class="line">rrf_docs = joiner.run(documents=[bm25_docs,dense_docs])[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;rrf:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> rrf_docs:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;content: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;doc.score&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="输出-5"><a href="#输出-5" class="headerlink" title="输出"></a>输出</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">content: Studying the history of the English language does not directly improve speaking skills.</span><br><span class="line">score: 0.9841269841269842</span><br><span class="line">content: Practicing with native speakers enhances English </span><br><span class="line">speaking proficiency.</span><br><span class="line">score: 0.9841269841269842</span><br><span class="line">content: Regular participation in debates and discussions refine public speaking </span><br><span class="line">skills in English.</span><br><span class="line">score: 0.9838709677419354</span><br></pre></td></tr></table></figure>
<h5 id="RRF计算：haystack使用k-61，并且进行了额外的放缩处理，-R-是排序列表的数量"><a href="#RRF计算：haystack使用k-61，并且进行了额外的放缩处理，-R-是排序列表的数量" class="headerlink" title="RRF计算：haystack使用k&#x3D;61，并且进行了额外的放缩处理，$|R|$是排序列表的数量"></a>RRF计算：haystack使用k&#x3D;61，并且进行了额外的放缩处理，$|R|$是排序列表的数量</h5><p>$$RRFscore(d\in D)&#x3D;\frac{k}{|R|}·\sum_{r\in R}\frac{1}{k+r(d)}$$</p>
<ul>
<li>Studying…：bm25的排序为1，dense的排序为3，因此：<br>$$61&#x2F;2\times(1&#x2F;(61+1)+1&#x2F;(61+3))&#x3D;0.984127$$</li>
<li>Practicing…：bm25的排序为3，dense的排序为1，因此：<br>$$61&#x2F;2\times(1&#x2F;(61+3)+1&#x2F;(61+1))&#x3D;0.984127$$</li>
<li>Regular…：bm25的排序为3，dense的排序为1，因此：<br>$$61&#x2F;2\times(1&#x2F;(61+2)+1&#x2F;(61+2))&#x3D;0.983871$$</li>
</ul>
<h3 id="重排序机制"><a href="#重排序机制" class="headerlink" title="重排序机制"></a>重排序机制</h3><ul>
<li>有些检索器速度快但效果不好(dense,sparse,bm25)，有些检索器速度慢但效果好(colbert,cross)</li>
<li>可以先用速度快的检索器先网罗一批候选文档，再用效果好的检索器重新排序。先快速粗读所有文档，找出一批看起来不错的文档，再精读候选文档，找出质量好的</li>
</ul>
<h4 id="例子-3"><a href="#例子-3" class="headerlink" title="例子"></a>例子</h4><h5 id="import-1"><a href="#import-1" class="headerlink" title="import"></a>import</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> haystack <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">from</span> haystack.document_stores.in_memory <span class="keyword">import</span> InMemoryDocumentStore</span><br><span class="line"><span class="keyword">from</span> haystack.components.retrievers.in_memory <span class="keyword">import</span> InMemoryBM25Retriever</span><br><span class="line"><span class="keyword">from</span> haystack.components.rankers <span class="keyword">import</span> TransformersSimilarityRanker</span><br></pre></td></tr></table></figure>
<h5 id="文档处理-1"><a href="#文档处理-1" class="headerlink" title="文档处理"></a>文档处理</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">&quot;What are effective strategies to improve English speaking skills?&quot;</span></span><br><span class="line">documents = [</span><br><span class="line">    Document(</span><br><span class="line">        content=<span class="string">&quot;Practicing with native speakers enhances English speaking proficiency.&quot;</span></span><br><span class="line">    ),</span><br><span class="line">    Document(</span><br><span class="line">        content=<span class="string">&quot;Daily vocabulary expansion is crucial for improving oral communication skills.&quot;</span></span><br><span class="line">    ),</span><br><span class="line">    Document(</span><br><span class="line">        content=<span class="string">&quot;Engaging in language exchange programs can significantly boost speaking abilities.&quot;</span></span><br><span class="line">    ),</span><br><span class="line">    Document(</span><br><span class="line">        content=<span class="string">&quot;Regular participation in debates and discussions refine public speaking skills in English.&quot;</span></span><br><span class="line">    ),</span><br><span class="line">    Document(</span><br><span class="line">        content=<span class="string">&quot;Studying the history of the English language does not directly improve speaking skills.&quot;</span></span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line">document_store = InMemoryDocumentStore()</span><br><span class="line">document_store.write_documents(documents)</span><br></pre></td></tr></table></figure>
<h5 id="bm25初步检索"><a href="#bm25初步检索" class="headerlink" title="bm25初步检索"></a>bm25初步检索</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bm25_retriever = InMemoryBM25Retriever(document_store=document_store)</span><br><span class="line">bm25_docs = bm25_retriever.run(query=query, top_k=<span class="number">4</span>)[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;bm25:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> bm25_docs:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;content: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;doc.score&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="输出-6"><a href="#输出-6" class="headerlink" title="输出"></a>输出</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bm25:</span><br><span class="line">content: Studying the history of the English language does not directly improve speaking skills.</span><br><span class="line">score: 3.1117211646172698</span><br><span class="line"></span><br><span class="line">content: Regular participation in debates and discussions refine public speaking skills in English.</span><br><span class="line">score: 2.443788686074245</span><br><span class="line"></span><br><span class="line">content: Practicing with native speakers enhances English speaking proficiency.</span><br><span class="line">score: 2.2622329312889553</span><br><span class="line"></span><br><span class="line">content: Daily vocabulary expansion is crucial for improving oral communication skills.</span><br><span class="line">score: 2.0359854825047066</span><br></pre></td></tr></table></figure>
<h5 id="重排序"><a href="#重排序" class="headerlink" title="重排序"></a>重排序</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">reranker = TransformersSimilarityRanker(model=<span class="string">&quot;cross-encoder/ms-marco-MiniLM-L-6-v2&quot;</span>)</span><br><span class="line">reranker.warm_up()</span><br><span class="line">reranked_docs = reranker.run(query=query, documents=bm25_docs, top_k=<span class="number">3</span>)[<span class="string">&quot;documents&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;reranker:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> reranked_docs:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;content: <span class="subst">&#123;doc.content&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;score: <span class="subst">&#123;doc.score&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="输出-7"><a href="#输出-7" class="headerlink" title="输出"></a>输出</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">reranker:</span><br><span class="line">content: Practicing with native speakers enhances English speaking proficiency.</span><br><span class="line">score: 0.769904375076294</span><br><span class="line"></span><br><span class="line">content: Studying the history of the English language does not directly improve </span><br><span class="line">speaking skills.</span><br><span class="line">score: 0.5486361384391785</span><br><span class="line"></span><br><span class="line">content: Daily vocabulary expansion is crucial for improving oral communication </span><br><span class="line">skills.</span><br><span class="line">score: 0.3509156107902527</span><br></pre></td></tr></table></figure>
<h3 id="上下文丰富"><a href="#上下文丰富" class="headerlink" title="上下文丰富"></a>上下文丰富</h3><p>小文档块的检索准确度更高，但丢失了更多上下文信息，因此可以在检索后丰富上下文来补偿</p>
<h4 id="上下文窗口扩展-Sentence-window-retrieval"><a href="#上下文窗口扩展-Sentence-window-retrieval" class="headerlink" title="上下文窗口扩展(Sentence window retrieval)"></a>上下文窗口扩展(Sentence window retrieval)</h4><ul>
<li>以小文档块为单位进行检索可以保证检索准确度，和相邻若干文档块合并形成大文档块可以保证信息量</li>
<li>翻阅书本时，突然扫到了重点，会下意识联系上下文看一看，看有没有额外的相关信息可以参考<br><img src="/pic/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/sentence_window.png" alt="图片错误"></li>
</ul>
<h4 id="自动合并检索-Auto-merging-retrieval"><a href="#自动合并检索-Auto-merging-retrieval" class="headerlink" title="自动合并检索(Auto-merging retrieval)"></a>自动合并检索(Auto-merging retrieval)</h4><ul>
<li>任何时候都进行上下文扩展并不合理，当检索命中的小文档块数量在大文档块中的占比达到一定阈值时(例如50%)，才进行合并</li>
<li>翻阅书本时，发现重点都聚集在某一章节，那这一章节可能都很重要<br><img src="/pic/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/auto_merging.png" alt="图片错误"></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://jihefx.github.io">及河</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://jihefx.github.io/2025/03/04/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/">https://jihefx.github.io/2025/03/04/LLM应用（一）：使用Haytack搭建RAG/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://jihefx.github.io" target="_blank">及河の小屋</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/RAG/">RAG</a><a class="post-meta__tags" href="/tags/Haystack/">Haystack</a></div><div class="post-share"><div class="social-share" data-image="/album/picture/blackmyth/20240829164105_1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>很高兴能帮到你</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/12/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AKerberos/" title="网络协议（三）：Kerberos"><img class="cover" src="/album/picture/blackmyth/20240901221718_1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">网络协议（三）：Kerberos</div></div><div class="info-2"><div class="info-item-1">一种验票机制</div></div></div></a><a class="pagination-related" href="/2025/03/26/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E7%9F%A5%E8%AF%86%E5%B1%82%E9%9D%A2%E7%9A%84%E6%BC%8F%E6%B4%9E%E6%A3%80%E6%B5%8BRAG/" title="LLM应用（二）：知识层面的漏洞检测RAG"><img class="cover" src="/album/picture/blackmyth/20240829164105_1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">LLM应用（二）：知识层面的漏洞检测RAG</div></div><div class="info-2"><div class="info-item-1">论文阅读</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/03/26/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E7%9F%A5%E8%AF%86%E5%B1%82%E9%9D%A2%E7%9A%84%E6%BC%8F%E6%B4%9E%E6%A3%80%E6%B5%8BRAG/" title="LLM应用（二）：知识层面的漏洞检测RAG"><img class="cover" src="/album/picture/blackmyth/20240829164105_1.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-26</div><div class="info-item-2">LLM应用（二）：知识层面的漏洞检测RAG</div></div><div class="info-2"><div class="info-item-1">论文阅读</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Twikoo</span><span id="switch-btn"></span><span class="second-comment">Giscus</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/Avatar_exusiai.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">及河</div><div class="author-info-description">肉和面包一起吃会更美味，生活和笔记一起丰富会更好玩。</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/jihefx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=l66voaCjr66mpNfm5rn0_Po" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://steamcommunity.com/id/jihefx-zhusiqi/" target="_blank" title="Steam"><i class="fas fa-brands fa-steam"></i></a><a class="social-icon" href="https://space.bilibili.com/1869898805" target="_blank" title="bilibili"><i class="fas fa-brands fa-bilibili"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><br> 1. 博主正在暑假，目前慢慢在看二进制安全的内容。
<br> 2. 该博客的文章、图片、代码参考网络资料，以个人学习为主，文献引用不全面的部分欢迎提醒，我会立刻补充，如果侵权将立即删掉。
<br> 3. 想交流技术的朋友可以加QQ联系：986748913
<br> 4. 右下角设置可以切换简/繁，白天/夜间。
<br> 5. 阅读文章时，可以在右下角开启阅读模式。
<br> 6. 评论账号支持Twikoo和Giscus，分别基于腾讯云和Github，在评论区右上角切换。能用github就尽量用Giscus，避免Twikoo限额。
<br> 7. 右下角的chatbot也可以交流，不过额度有限，建议通过评论区或QQ联系。
</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Document"><span class="toc-number">1.</span> <span class="toc-text">Document</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Component"><span class="toc-number">2.</span> <span class="toc-text">Component</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pipeline"><span class="toc-number">3.</span> <span class="toc-text">Pipeline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Converter"><span class="toc-number">4.</span> <span class="toc-text">Converter</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spliter"><span class="toc-number">5.</span> <span class="toc-text">Spliter</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#DocumentSplitter"><span class="toc-number">5.1.</span> <span class="toc-text">DocumentSplitter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NLTKDocumentSplitter"><span class="toc-number">5.2.</span> <span class="toc-text">NLTKDocumentSplitter</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Retreiver"><span class="toc-number">6.</span> <span class="toc-text">Retreiver</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BM25Retriever%E5%8E%9F%E7%90%86"><span class="toc-number">6.1.</span> <span class="toc-text">BM25Retriever原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bert"><span class="toc-number">6.2.</span> <span class="toc-text">Bert</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DenseEmbeddingRetriever-%E6%96%87%E6%9C%AC%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.3.</span> <span class="toc-text">DenseEmbeddingRetriever: 文本嵌入模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DenseEmbeddingRetriever-%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">6.4.</span> <span class="toc-text">DenseEmbeddingRetriever: 相似度计算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">6.4.1.</span> <span class="toc-text">例子</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E6%9F%A5%E8%AF%A2"><span class="toc-number">6.4.1.1.</span> <span class="toc-text">处理查询</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BE%93%E5%87%BA"><span class="toc-number">6.4.1.2.</span> <span class="toc-text">输出</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">6.4.2.</span> <span class="toc-text">优缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SimilarityReranker-%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.5.</span> <span class="toc-text">SimilarityReranker: 相似度计算模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90-1"><span class="toc-number">6.5.1.</span> <span class="toc-text">例子</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BE%93%E5%87%BA-1"><span class="toc-number">6.5.1.1.</span> <span class="toc-text">输出</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9-1"><span class="toc-number">6.6.</span> <span class="toc-text">优缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Simple-RAG"><span class="toc-number">7.</span> <span class="toc-text">Simple RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E6%96%87%E6%A1%A3"><span class="toc-number">7.1.</span> <span class="toc-text">处理文档</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E6%9F%A5%E8%AF%A2-1"><span class="toc-number">7.2.</span> <span class="toc-text">处理查询</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">7.3.</span> <span class="toc-text">测试</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#What-is-the-motto-of-Nanjing-University%EF%BC%9F"><span class="toc-number">7.3.1.</span> <span class="toc-text">What is the motto of Nanjing University？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#What-is-the-song-of-Nanjing-University%EF%BC%9F"><span class="toc-number">7.3.2.</span> <span class="toc-text">What is the song of Nanjing University？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E4%B8%80%E4%BA%9B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">7.3.3.</span> <span class="toc-text">问一些大模型不知道的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Chatgpt-answer"><span class="toc-number">7.3.3.1.</span> <span class="toc-text">Chatgpt answer</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#RAG-answer"><span class="toc-number">7.3.3.2.</span> <span class="toc-text">RAG answer</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advanced-RAG-%E6%A3%80%E7%B4%A2%E7%BB%93%E6%9E%9C%E5%90%88%E5%B9%B6"><span class="toc-number">8.</span> <span class="toc-text">Advanced RAG: 检索结果合并</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E5%90%88%E5%B9%B6-Weight-Merge"><span class="toc-number">8.1.</span> <span class="toc-text">权重合并(Weight Merge)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RRF-%E5%80%92%E6%8E%92%E8%9E%8D%E5%90%88"><span class="toc-number">8.2.</span> <span class="toc-text">RRF(倒排融合)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90-2"><span class="toc-number">8.2.1.</span> <span class="toc-text">例子</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#import"><span class="toc-number">8.2.1.1.</span> <span class="toc-text">import</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86"><span class="toc-number">8.2.1.2.</span> <span class="toc-text">文档处理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#bm25%E6%A3%80%E7%B4%A2"><span class="toc-number">8.2.1.3.</span> <span class="toc-text">bm25检索</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BE%93%E5%87%BA-2"><span class="toc-number">8.2.1.4.</span> <span class="toc-text">输出</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#dense-embedding%E6%A3%80%E7%B4%A2"><span class="toc-number">8.2.1.5.</span> <span class="toc-text">dense embedding检索</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BE%93%E5%87%BA-3"><span class="toc-number">8.2.1.6.</span> <span class="toc-text">输出</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E5%90%88%E5%B9%B6"><span class="toc-number">8.2.1.7.</span> <span class="toc-text">权重合并</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BE%93%E5%87%BA-4"><span class="toc-number">8.2.1.8.</span> <span class="toc-text">输出</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#RRF%E5%90%88%E5%B9%B6"><span class="toc-number">8.2.1.9.</span> <span class="toc-text">RRF合并</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BE%93%E5%87%BA-5"><span class="toc-number">8.2.1.10.</span> <span class="toc-text">输出</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#RRF%E8%AE%A1%E7%AE%97%EF%BC%9Ahaystack%E4%BD%BF%E7%94%A8k-61%EF%BC%8C%E5%B9%B6%E4%B8%94%E8%BF%9B%E8%A1%8C%E4%BA%86%E9%A2%9D%E5%A4%96%E7%9A%84%E6%94%BE%E7%BC%A9%E5%A4%84%E7%90%86%EF%BC%8C-R-%E6%98%AF%E6%8E%92%E5%BA%8F%E5%88%97%E8%A1%A8%E7%9A%84%E6%95%B0%E9%87%8F"><span class="toc-number">8.2.1.11.</span> <span class="toc-text">RRF计算：haystack使用k&#x3D;61，并且进行了额外的放缩处理，$|R|$是排序列表的数量</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E6%8E%92%E5%BA%8F%E6%9C%BA%E5%88%B6"><span class="toc-number">8.3.</span> <span class="toc-text">重排序机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90-3"><span class="toc-number">8.3.1.</span> <span class="toc-text">例子</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#import-1"><span class="toc-number">8.3.1.1.</span> <span class="toc-text">import</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86-1"><span class="toc-number">8.3.1.2.</span> <span class="toc-text">文档处理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#bm25%E5%88%9D%E6%AD%A5%E6%A3%80%E7%B4%A2"><span class="toc-number">8.3.1.3.</span> <span class="toc-text">bm25初步检索</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BE%93%E5%87%BA-6"><span class="toc-number">8.3.1.4.</span> <span class="toc-text">输出</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%87%8D%E6%8E%92%E5%BA%8F"><span class="toc-number">8.3.1.5.</span> <span class="toc-text">重排序</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BE%93%E5%87%BA-7"><span class="toc-number">8.3.1.6.</span> <span class="toc-text">输出</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E4%B8%B0%E5%AF%8C"><span class="toc-number">8.4.</span> <span class="toc-text">上下文丰富</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AA%97%E5%8F%A3%E6%89%A9%E5%B1%95-Sentence-window-retrieval"><span class="toc-number">8.4.1.</span> <span class="toc-text">上下文窗口扩展(Sentence window retrieval)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%90%88%E5%B9%B6%E6%A3%80%E7%B4%A2-Auto-merging-retrieval"><span class="toc-number">8.4.2.</span> <span class="toc-text">自动合并检索(Auto-merging retrieval)</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/26/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E7%9F%A5%E8%AF%86%E5%B1%82%E9%9D%A2%E7%9A%84%E6%BC%8F%E6%B4%9E%E6%A3%80%E6%B5%8BRAG/" title="LLM应用（二）：知识层面的漏洞检测RAG"><img src="/album/picture/blackmyth/20240829164105_1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM应用（二）：知识层面的漏洞检测RAG"/></a><div class="content"><a class="title" href="/2025/03/26/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E7%9F%A5%E8%AF%86%E5%B1%82%E9%9D%A2%E7%9A%84%E6%BC%8F%E6%B4%9E%E6%A3%80%E6%B5%8BRAG/" title="LLM应用（二）：知识层面的漏洞检测RAG">LLM应用（二）：知识层面的漏洞检测RAG</a><time datetime="2025-03-26T05:21:42.000Z" title="发表于 2025-03-26 13:21:42">2025-03-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/04/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/" title="LLM应用（一）：使用Haytack搭建RAG"><img src="/album/picture/blackmyth/20240829164105_1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM应用（一）：使用Haytack搭建RAG"/></a><div class="content"><a class="title" href="/2025/03/04/LLM%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8Haytack%E6%90%AD%E5%BB%BARAG/" title="LLM应用（一）：使用Haytack搭建RAG">LLM应用（一）：使用Haytack搭建RAG</a><time datetime="2025-03-04T07:51:13.000Z" title="发表于 2025-03-04 15:51:13">2025-03-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/12/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AKerberos/" title="网络协议（三）：Kerberos"><img src="/album/picture/blackmyth/20240901221718_1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="网络协议（三）：Kerberos"/></a><div class="content"><a class="title" href="/2024/12/12/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AKerberos/" title="网络协议（三）：Kerberos">网络协议（三）：Kerberos</a><time datetime="2024-12-12T03:36:13.000Z" title="发表于 2024-12-12 11:36:13">2024-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/27/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9ASSL/" title="网络协议（二）：SSL"><img src="/album/picture/blackmyth/20240901221718_1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="网络协议（二）：SSL"/></a><div class="content"><a class="title" href="/2024/11/27/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9ASSL/" title="网络协议（二）：SSL">网络协议（二）：SSL</a><time datetime="2024-11-27T02:33:26.000Z" title="发表于 2024-11-27 10:33:26">2024-11-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/13/%E6%B8%97%E9%80%8F%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9Apwn%E5%85%A5%E9%97%A8/" title="渗透笔记（一）：pwn入门"><img src="/album/picture/blackmyth/20241005210930_1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="渗透笔记（一）：pwn入门"/></a><div class="content"><a class="title" href="/2024/11/13/%E6%B8%97%E9%80%8F%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9Apwn%E5%85%A5%E9%97%A8/" title="渗透笔记（一）：pwn入门">渗透笔记（一）：pwn入门</a><time datetime="2024-11-13T07:41:24.000Z" title="发表于 2024-11-13 15:41:24">2024-11-13</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/album/picture/actwallpaper/v2-2c7d04494127743c5c51fd0cf948bd9a_r.jpg);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2023 - 2025 By 及河</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.1</a></span></div><div class="footer_custom_text">一个普通的笔记<a href="https://jihefx.github.io/">Blog</a>，很高兴遇到你！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="聊天" style="display:none"><i class="fas fa-message"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://jihe-blog.hf.space',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = (el = document, path = location.pathname) => {
    twikoo.init({
      el: el.querySelector('#twikoo-wrap'),
      envId: 'https://jihe-blog.hf.space',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      },
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    

    isShuoshuo && (window.shuoshuoComment.destroyTwikoo = () => {
      if (el.children.length) {
        el.innerHTML = ''
        el.classList.add('no-comment')
      }
    })
  }

  const loadTwikoo = (el, path) => {
    if (typeof twikoo === 'object') setTimeout(() => init(el, path), 0)
    else btf.getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(() => init(el, path))
  }

  if (isShuoshuo) {
    'Twikoo' === 'Twikoo'
      ? window.shuoshuoComment = { loadComment: loadTwikoo }
      : window.loadOtherComment = loadTwikoo
    return
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'JiHeFX/giscus',
      'data-repo-id': 'R_kgDOPI6vOg',
      'data-category-id': 'DIC_kwDOPI6vOs4CspKZ',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Twikoo' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Twikoo' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script>(() => {
  btf.getScript('//code.tidio.co/6nkqfwwznxdkacehf0vbgl2fbihhd7zz.js').then(() => {
    const isChatBtn = true
    const isChatHideShow = false

    if (isChatBtn) {
      let isShow = false
      const close = () => {
        window.tidioChatApi.hide()
        isShow = false
      }
      
      const open = () => {
        window.tidioChatApi.open()
        window.tidioChatApi.show()
        isShow = true
      }

      const onTidioChatApiReady = () => {
        window.tidioChatApi.hide()
        window.tidioChatApi.on("close", close)
      }
      if (window.tidioChatApi) {
        window.tidioChatApi.on("ready", onTidioChatApiReady)
      } else {
        document.addEventListener("tidioChat-ready", onTidioChatApiReady)
      }

      window.chatBtnFn = () => {
        if (!window.tidioChatApi) return
        isShow ? close() : open()
      }

      document.getElementById('chat-btn').style.display = 'block'

    } else if (isChatHideShow) {
      window.chatBtn = {
        hide: () => window.tidioChatApi && window.tidioChatApi.hide(),
        show: () => window.tidioChatApi && window.tidioChatApi.show()
      }
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div></body></html>